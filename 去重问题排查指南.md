# Item 去重问题排查指南

## 问题现象
- ✅ csv_pipeline.py 已经修复（字段缓存）
- ❌ 但还是看到重复存储的数据

## 根本原因
**不是 csv_pipeline.py 的问题，而是 Item 去重没有启用！**

---

## 快速排查（5分钟）

### 步骤 1：找到你的 setting.py

```bash
# 如果你在 tests/test-pipeline 目录下运行爬虫
cat tests/test-pipeline/setting.py | grep "ITEM_FILTER"

# 如果你有独立的项目
cat your_project/setting.py | grep "ITEM_FILTER"
```

### 步骤 2：检查当前配置

查看这两行：
```python
ITEM_FILTER_ENABLE = False       # ❌ 如果是 False，说明去重没启用
ITEM_FILTER_SETTING = dict(...)
```

### 步骤 3：启用去重

修改为：
```python
ITEM_FILTER_ENABLE = True  # ✅ 改这里！

ITEM_FILTER_SETTING = dict(
    filter_type=3,  # 临时去重（推荐用于定期爬虫）
    expire_time=86400  # 24小时后自动清除
)
```

### 步骤 4：重新运行爬虫

```bash
python your_spider.py
```

查看日志中是否出现：
```
待入库数据 100 条， 重复 5 条，实际待入库数据 95 条
       ↑ 看到这个说明去重生效了！
```

---

## 详细分析

### 去重的工作原理

```
Item → 生成 fingerprint (MD5哈希)
       ↓
       查询去重库：这个 fingerprint 是否存在？
       ├─ 存在 → 过滤掉（不写入 CSV）
       └─ 不存在 → 保留（写入 CSV，并记录到去重库）
```

### 为什么启用去重后 CSV 中仍然有重复

可能的原因：

| 原因 | 症状 | 解决方案 |
|------|------|--------|
| Item 的唯一标识不对 | 应该过滤但没过滤 | 检查 Item 的所有字段是否有值 |
| unique_key 设置错误 | 只想用部分字段判断重复 | 在 Item 中明确指定 unique_key |
| 去重库清除时间太短 | 旧数据被清除了 | 增加 expire_time |
| 去重库清除时间太长 | 新字段改变的数据被认为重复 | 减少 expire_time 或改用 filter_type=4 |

---

## 配置参数说明

### filter_type（去重方式）

```python
ITEM_FILTER_SETTING = dict(
    filter_type=1  # BloomFilter - 永久去重
    # 适合：爬虫只运行一次，或数据不更新
    # 缺点：占用磁盘空间，容易占满
)

ITEM_FILTER_SETTING = dict(
    filter_type=2  # MemoryFilter - 内存去重
    # 适合：单次爬虫运行，内存足够
    # 缺点：程序退出后数据丢失，无法跨进程
)

ITEM_FILTER_SETTING = dict(
    filter_type=3,  # ExpireFilter - 临时去重
    # 适合：定期爬虫（推荐！）
    expire_time=86400  # 24小时后自动清除
    # 缺点：需要设置合理的过期时间
)

ITEM_FILTER_SETTING = dict(
    filter_type=4  # LiteFilter - 轻量去重
    # 适合：轻量级项目
    # 缺点：去重效果可能不如其他方式
)
```

### 推荐配置

**如果你每天爬一次**：
```python
ITEM_FILTER_ENABLE = True
ITEM_FILTER_SETTING = dict(
    filter_type=3,
    expire_time=86400  # 24小时
)
```

**如果你每小时爬一次**：
```python
ITEM_FILTER_ENABLE = True
ITEM_FILTER_SETTING = dict(
    filter_type=3,
    expire_time=3600  # 1小时
)
```

**如果你只爬一次**：
```python
ITEM_FILTER_ENABLE = True
ITEM_FILTER_SETTING = dict(
    filter_type=1  # BloomFilter 永久去重
)
```

---

## Item 的 fingerprint 是如何计算的

### 默认行为（使用所有字段）

```python
class MyItem(Item):
    class Meta:
        collection = "products"

item = MyItem()
item.url = "https://example.com/product/123"
item.name = "iPhone"
item.price = "9999"

# fingerprint = MD5(MD5("https://example.com/product/123" + "iPhone" + "9999"))
# 如果任何字段不同，fingerprint 就会不同
```

### 自定义 unique_key（只使用特定字段）

```python
class MyItem(Item):
    class Meta:
        collection = "products"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.unique_key = "url"  # 只用 url 判断重复

item = MyItem()
item.url = "https://example.com/product/123"
item.name = "iPhone"
item.price = "9999"

# fingerprint = MD5("https://example.com/product/123")
# 即使 name 和 price 变了，只要 url 相同就认为重复
```

### 使用多个字段的 unique_key

```python
class MyItem(Item):
    class Meta:
        collection = "products"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.unique_key = ("url", "date")  # 用 url 和 date 组合判断

item = MyItem()
item.url = "https://example.com/product/123"
item.date = "2025-11-07"
item.price = "9999"

# fingerprint = MD5("https://example.com/product/123" + "2025-11-07")
# url 或 date 相同就认为重复
```

---

## 验证去重是否工作

### 检查 1：日志中是否有"重复"信息

启用去重后，运行爬虫：

```bash
python your_spider.py 2>&1 | grep "重复"
```

**预期输出**：
```
待入库数据 100 条， 重复 5 条，实际待入库数据 95 条
```

如果没有看到这个日志，说明去重没启用。

### 检查 2：CSV 文件的行数

```bash
# 第一次运行
python your_spider.py
wc -l data/csv/users.csv  # 例如：101 行（1行表头+100行数据）

# 第二次运行（重复爬取相同数据）
python your_spider.py
wc -l data/csv/users.csv  # 如果有去重：还是 101 行
                          # 如果没去重：207 行（100+100+1头）
```

### 检查 3：查看去重库

```python
# 临时查看去重库中的数据（仅供调试）
from feapder.dedup import Dedup

dedup = Dedup(name="my_spider")
# 可以查看去重库中有多少条数据
```

---

## 常见问题

### Q1：启用去重后，日志中还是没有"重复"信息

**A**：可能的原因：
1. 你的 Item 没有设置任何值（fingerprint=None）
2. 你每次爬到的数据都不一样
3. 去重库被清除了

**检查方法**：
```python
# 在你的爬虫中添加调试
def parse(self, request, response):
    item = MyItem()
    item.url = request.url
    item.name = response.xpath('//title/text()').extract_first()
    
    # 调试：打印 fingerprint
    print(f"Item fingerprint: {item.fingerprint}")
    print(f"Item data: {item.to_dict}")
    
    yield item
```

### Q2：CSV 中还是有重复，怎么办

**A**：执行以下检查：

1. **确认 ITEM_FILTER_ENABLE = True**
```bash
grep "ITEM_FILTER_ENABLE" your_setting.py
```

2. **清除旧的去重库数据**
```bash
# 如果使用 Redis 存储去重库
redis-cli
> KEYS "*dedup*"  # 查看所有去重库
> DEL <key>       # 删除去重库
```

3. **重新运行爬虫**
```bash
python your_spider.py
```

### Q3：去重库占用太多空间

**A**：改用 filter_type=4（轻量去重）：
```python
ITEM_FILTER_SETTING = dict(
    filter_type=4  # LiteFilter - 轻量去重
)
```

或改用定时清除：
```python
ITEM_FILTER_SETTING = dict(
    filter_type=3,
    expire_time=86400  # 每天清除一次
)
```

---

## 总结

| 检查项 | 操作 |
|--------|------|
| 是否启用去重 | `ITEM_FILTER_ENABLE = True` |
| 选择去重方式 | `filter_type=3` （推荐用于定期爬虫） |
| 设置过期时间 | `expire_time=86400` （24小时） |
| 运行爬虫 | `python your_spider.py` |
| 查看日志 | 搜索"重复"关键字 |
| 验证 CSV | 检查行数和内容 |

**如果还有问题，提供以下信息**：
1. 你的 setting.py 中 ITEM_FILTER_* 的配置
2. 运行爬虫时的日志输出
3. CSV 文件中重复数据的具体情况

