# Item 去重机制分析与修复

## 问题诊断

你发现 CSV 中依然有重复存储的数据，这不是 `csv_pipeline.py` 的问题，而是你**没有正确启用 Item 去重机制**或**Item 去重被破坏了**。

---

## Item 去重的完整流程

### 1. 流程概览

```
爬虫 yield item
  ↓
Item 进入 ItemBuffer 队列
  ↓
ItemBuffer.flush() 周期性调用
  ↓
__add_item_to_db() 处理
  ├─ ✅ 第1步：__dedup_items() - 去重（如果 ITEM_FILTER_ENABLE=True）
  │   ├─ 生成 fingerprint（每个item的唯一标识）
  │   ├─ 查询去重库，判断是否存在
  │   └─ 过滤掉重复的 items
  │
  ├─ 第2步：__pick_items() - 按表分组
  │
  └─ 第3步：__export_to_db() - 调用各个 pipeline
      └─ csv_pipeline.save_items()
          └─ 只会保存去重后的数据

后续：
  if export_success:
      ├─ 去重入库：dedup.add(items_fingerprints) - 记录已处理过的fingerprints
      └─ 删除请求：redis_db.zrem()
```

### 2. 关键信息

**去重的三个关键点**：

1. **去重前检查** (item_buffer.py:287-288)
```python
if setting.ITEM_FILTER_ENABLE:
    items, items_fingerprints = self.__dedup_items(items, items_fingerprints)
    # items 被过滤，重复的被移除
```

2. **去重指纹计算** (item.py:127-138)
```python
@property
def fingerprint(self):
    args = []
    for key, value in self.to_dict.items():
        if value:
            if (self.unique_key and key in self.unique_key) or not self.unique_key:
                args.append(str(value))

    if args:
        args = sorted(args)
        return tools.get_md5(*args)  # 生成 MD5 哈希
    else:
        return None
```

3. **去重后入库** (item_buffer.py:348-350)
```python
if export_success:
    if setting.ITEM_FILTER_ENABLE:
        if items_fingerprints:
            self.__class__.dedup.add(items_fingerprints, skip_check=True)
            # 只有成功导出的数据才会被添加到去重库
```

---

## 为什么你会看到重复数据

### 原因 1：ITEM_FILTER_ENABLE 没有开启

**当前状态**（在 `feapder/setting.py`）：
```python
ITEM_FILTER_ENABLE = False  # ❌ 关闭了
```

**结果**：
- ItemBuffer 根本不执行去重逻辑
- 所有数据直接写入 CSV
- 重复的数据被保存

**修复方法**：在你的 setting.py 中改为：
```python
ITEM_FILTER_ENABLE = True  # ✅ 启用
```

### 原因 2：Item 没有定义 unique_key

**概念**：
- `fingerprint` 是通过 Item 的所有属性值生成的唯一标识
- 默认情况下，使用所有非空属性值来生成 fingerprint
- 可以通过 `unique_key` 指定只使用某些属性来生成 fingerprint

**例子**：

```python
# 不指定 unique_key（使用所有属性）
class MyItem(Item):
    class Meta:
        collection = "products"

item = MyItem()
item.url = "https://example.com/product/123"
item.name = "iPhone"
item.price = "9999"

# fingerprint = MD5(hash("9999", "iPhone", "https://example.com/product/123"))
# 如果任何一个属性不同，fingerprint 就会不同
```

```python
# 指定 unique_key（只使用 url 属性）
class MyItem(Item):
    class Meta:
        collection = "products"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.unique_key = "url"  # 只用 url 来判断重复

item = MyItem()
item.url = "https://example.com/product/123"
item.name = "iPhone"
item.price = "9999"

# fingerprint = MD5(hash("https://example.com/product/123"))
# 即使 name 和 price 变化，只要 url 相同就认为是重复的
```

### 原因 3：去重库的生命周期问题

去重库有不同的类型，决定了数据什么时候被清除：

```python
ITEM_FILTER_SETTING = dict(
    filter_type=1  # ❌ 问题！默认是 BloomFilter（永久去重）
)
```

| filter_type | 说明 | 使用场景 |
|-----------|------|--------|
| 1 | BloomFilter（永久去重）| 一次性爬虫，从不重爬 |
| 2 | MemoryFilter（内存去重）| 单次运行，内存大小够 |
| 3 | ExpireFilter（临时去重）| 定期爬虫，按天/月清除 |
| 4 | LiteFilter（轻量去重）| 轻量级，占用资源少 |

**如果你是定期爬虫（例如每天爬一次）**，应该用：
```python
ITEM_FILTER_SETTING = dict(
    filter_type=3,  # 临时去重（推荐）
    expire_time=86400  # 24小时后自动清除
)
```

---

## csv_pipeline.py 中的问题

现在让我检查 `csv_pipeline.py` 是否正确配合去重机制：

### 关键发现：csv_pipeline 不处理去重

```python
def save_items(self, table, items: List[Dict]) -> bool:
    # items 已经是去重后的数据（由 ItemBuffer 过滤）
    # csv_pipeline 不需要做任何额外的去重处理
    # 只需要原样保存即可

    # 当前的实现是正确的！
    writer.writerows(items)  # items 已经被去重了
    return True
```

**结论**：
- ✅ `csv_pipeline.py` 的实现是正确的
- ✅ 它正确地保存了 ItemBuffer 传过来的数据
- ❌ 问题出在 ItemBuffer 没有执行去重（因为 ITEM_FILTER_ENABLE=False）

---

## 完整的修复清单

### 步骤 1：启用 Item 去重

编辑你的 `setting.py`（最可能是 `tests/test-pipeline/setting.py` 或项目根目录的 setting.py）：

```python
# 修改前
ITEM_FILTER_ENABLE = False

# 修改后
ITEM_FILTER_ENABLE = True
```

### 步骤 2：配置去重方式

根据你的需求选择合适的去重方式：

```python
# 方案 A：一次性爬虫（从不重爬）
ITEM_FILTER_SETTING = dict(
    filter_type=1  # BloomFilter（永久去重）
)

# 方案 B：定期爬虫（推荐）
ITEM_FILTER_SETTING = dict(
    filter_type=3,  # ExpireFilter（临时去重）
    expire_time=86400  # 24小时后清除
)

# 方案 C：内存去重（单次运行）
ITEM_FILTER_SETTING = dict(
    filter_type=2  # MemoryFilter
)
```

### 步骤 3：（可选）指定 unique_key

如果你想用特定字段来判断重复（例如只按 URL 判断），可以在 Item 类中设置：

```python
class MyItem(Item):
    class Meta:
        collection = "products"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # 只使用 url 字段来判断是否重复
        self.unique_key = "url"
```

### 步骤 4：验证是否生效

启用去重后，运行爬虫，查看日志：

```
✅ 正常日志（启用了去重）：
INFO: 待入库数据 100 条， 重复 5 条，实际待入库数据 95 条
      ↑ 这说明去重成功了，有5条重复被过滤

❌ 不正常日志（没启用去重）：
INFO: 共导出 100 条数据 到 users.csv
     ↑ 没有看到"重复"的信息，说明去重没启用
```

---

## 工作流程验证

### 修复前（ITEM_FILTER_ENABLE = False）

```
第1天爬虫运行：
  ├─ 爬取 100 条数据 (URL: product/1, product/2, ..., product/100)
  ├─ 写入 CSV: 100 行
  └─ 去重库: 未启用，什么都没记录

第2天爬虫再运行（爬到了部分重复的数据）：
  ├─ 爬取 100 条数据 (URL: product/50-100, product/101-150)
  ├─ ItemBuffer 没有执行去重（ITEM_FILTER_ENABLE=False）
  ├─ 直接调用 csv_pipeline.save_items()
  └─ 写入 CSV: 又增加了 100 行 ❌ 其中 50 行是重复的

最终 CSV：
  product/1 ... product/100 ... product/50-100 (重复!) ... product/101-150
             ↑ 第1天的数据
             ↑ 第2天的数据（包含重复）
```

### 修复后（ITEM_FILTER_ENABLE = True）

```
第1天爬虫运行：
  ├─ 爬取 100 条数据
  ├─ ItemBuffer.__dedup_items()：检查去重库，全部新数据 ✅
  ├─ 保存 100 行到 CSV
  └─ 去重库.add()：记录这 100 条数据的 fingerprints

第2天爬虫再运行（爬到了部分重复的数据）：
  ├─ 爬取 100 条数据 (URL: product/50-100, product/101-150)
  ├─ ItemBuffer.__dedup_items()：
  │  ├─ product/50-100 的 fingerprints 查询去重库 → 存在 ❌ 过滤掉
  │  └─ product/101-150 的 fingerprints 查询去重库 → 不存在 ✅ 保留
  ├─ 去重后只有 50 条新数据
  ├─ 调用 csv_pipeline.save_items() → 保存 50 条
  └─ 去重库.add()：添加新数据的 fingerprints

最终 CSV：
  product/1 ... product/100 ... product/101-150
             ↑ 第1天的数据
             ↑ 第2天的新数据（重复的被过滤了！）
```

---

## 关键要点总结

| 步骤 | 执行者 | 是否修改 csv_pipeline |
|------|-------|----------------------|
| 1. 生成 fingerprint | Item 类 | ❌ 不需要 |
| 2. 去重判断 | ItemBuffer | ❌ 不需要 |
| 3. 过滤重复数据 | ItemBuffer | ❌ 不需要 |
| 4. 保存去重后的数据 | csv_pipeline | ✅ 已正确实现 |
| 5. 记录到去重库 | ItemBuffer | ❌ 不需要 |

**结论**：
- ✅ `csv_pipeline.py` 的代码已正确实现
- ✅ 它会自动保存 ItemBuffer 去重后的数据
- ❌ 问题在于你的 setting.py 中没有启用去重
- ✅ 启用去重后，csv_pipeline 会自动接收去重后的数据

---

## 检查清单

### ✅ 检查点 1：查看你的 setting.py

```bash
grep -n "ITEM_FILTER_ENABLE" your_setting.py
```

**预期结果**：
```
ITEM_FILTER_ENABLE = True  # ✅ 应该是 True
```

### ✅ 检查点 2：查看去重配置

```bash
grep -A2 "ITEM_FILTER_SETTING" your_setting.py
```

**预期结果**：
```python
ITEM_FILTER_SETTING = dict(
    filter_type=3,  # ✅ 或 1、2、4，取决于场景
    expire_time=86400
)
```

### ✅ 检查点 3：运行爬虫，查看日志

```bash
# 运行爬虫
python your_spider.py

# 查看日志中是否有"重复"的信息
# grep "重复" your.log
```

**预期日志**：
```
待入库数据 100 条， 重复 5 条，实际待入库数据 95 条
```

### ✅ 检查点 4：验证 CSV 中没有重复

```bash
# 统计 CSV 中某个关键字段的行数
cut -d',' -f2 data/csv/users.csv | sort | uniq -d | wc -l

# 如果输出是 0，说明没有重复 ✅
# 如果输出 > 0，说明还有重复 ❌
```

---

## 总结

你看到的重复存储问题**不是 csv_pipeline.py 的问题**，而是：

1. **ITEM_FILTER_ENABLE 没有启用** ← 最可能的原因
2. Item 的 unique_key 设置不当
3. 去重库的类型选择不当

**立即修复**：
1. 找到你的 setting.py
2. 改 `ITEM_FILTER_ENABLE = False` → `ITEM_FILTER_ENABLE = True`
3. 重新运行爬虫
4. 查看日志中是否出现"重复"的信息
5. 验证 CSV 中是否没有重复数据

如果还有问题，我可以帮你进一步调试！
